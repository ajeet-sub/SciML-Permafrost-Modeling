{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1YgsBZ_y0-61e5umHCrXeOsOqNpwYtkJ7","authorship_tag":"ABX9TyP+bcmt5Dak3PCBIMN4tRiJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgJMrxUVCYj9","executionInfo":{"status":"ok","timestamp":1732932123982,"user_tz":300,"elapsed":641,"user":{"displayName":"Yeonwoo","userId":"10258336815477877515"}},"outputId":"fe1b4d74-f760-41cb-85e7-1fcca3c82517"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-13-355ed2301072>:46: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  daily_reshaped = reshaped.resample('D').interpolate(method=\"linear\").fillna(method=\"bfill\").fillna(method='ffill')\n"]},{"output_type":"stream","name":"stdout","text":["Daily table saved to: /content/drive/MyDrive/GTNP_Borehole_Dataset/Cleaned/Observations_daily_interpolated.csv\n"]}],"source":["import pandas as pd\n","import numpy as np\n","\n","\n","# Load the uploaded CSV file to examine its structure\n","drive_path = '/content/drive/MyDrive/GTNP_Borehole_Dataset/Cleaned'\n","file_path = drive_path + '/consolidated_data_master.csv'\n","data = pd.read_csv(file_path)\n","\n","reshaped = (\n","    data.set_index([\"Date\", \"Depth\"])[[\"Avg Ground Temp\"]]\n","    .apply(lambda row: row.iloc[0], axis=1)  # Unpack the array and get the value\n","    .unstack(level=1)  # Unstack by Depth to make Depth the columns\n",")\n","\n","# Convert the temperatures from Kelvin to Celsius\n","reshaped = reshaped - 273.15\n","\n","# Rename columns with depth values for clarity\n","reshaped.columns = [f\"{col}m\" for col in reshaped.columns]\n","\n","# Add the \"Date\" index back as a column for saving or displaying\n","reshaped.reset_index(inplace=True)\n","\n","# Extract unique monthly air temperatures and convert to Celsius\n","monthly_air_temp = (\n","    data.groupby(\"Date\")[\"Air Temp\"].first() - 273.15\n",").reset_index()\n","\n","# Rename the column for clarity\n","monthly_air_temp.rename(columns={\"Air Temp\": \"Air\"}, inplace=True)\n","\n","# Merge the air temperature data into the reshaped DataFrame\n","reshaped = pd.merge(monthly_air_temp, reshaped, on=\"Date\")\n","\n","# Rename the Date column to Time\n","reshaped.rename(columns={\"Date\": \"Time\"}, inplace=True)\n","\n","# Convert the Time column to a datetime format\n","reshaped['Time'] = pd.to_datetime(reshaped['Time'])\n","\n","# Set Time as the index\n","reshaped.set_index('Time', inplace=True)\n","\n","# Resample the data to a daily frequency and interpolate\n","daily_reshaped = reshaped.resample('D').interpolate(method=\"linear\").fillna(method=\"bfill\").fillna(method='ffill')\n","\n","# Reset the index to include Time as a column\n","daily_reshaped.reset_index(inplace=True)\n","\n","# Save the daily table to a CSV file\n","output_path = drive_path + '/Observations_daily_interpolated.csv'  # Update with your desired save path\n","daily_reshaped.to_csv(output_path, index=False)\n","\n","# Print confirmation\n","print(f\"Daily table saved to: {output_path}\")\n","\n"]},{"cell_type":"code","source":["\"\"\"\n","# Define constants\n","rho_soil_particles = 2650  # kg/m³\n","c_soil = 800  # J/kg·K\n","rho_water = 1000  # kg/m³\n","c_water = 4186  # J/kg·K\n","rho_bulk = 1500  # kg/m³\n","\n","# Calculate porosity\n","n = 1 - (rho_bulk / rho_soil_particles)\n","\n","# Extract soil moisture as a numpy array for vectorized computation\n","theta = data['Soil Moisture'].values  # Volumetric water content\n","\n","# Vectorized thermal conductivity calculation\n","k_dry = 0.25  # W/m·K\n","k_sat = 1.5  # W/m·K\n","thermal_conductivity = k_dry + (k_sat - k_dry) * theta\n","\n","# Vectorized volumetric heat capacity calculation\n","heat_capacity = (rho_bulk * c_soil) * (1 - theta) + (rho_water * c_water) * theta\n","\n","# Soil density is constant as rho_bulk\n","density = np.full_like(theta, rho_bulk)\n","\n","# Combine results into a dataframe\n","thermal_properties = pd.DataFrame({\n","    'Depth': data['Depth'],\n","    'Thermal Conductivity': thermal_conductivity,\n","    'Heat Capacity': heat_capacity,\n","    'Density': density\n","})\n","\n","# Handle duplicate depths: aggregate by mean if required\n","aggregated_properties = thermal_properties.groupby('Depth').mean().reset_index()\n","\n","# Save the aggregated results to a file (if GIPL needs unique depths)\n","aggregated_soil_properties_file = drive_path + '/aggregated_soil_properties_input.txt'\n","aggregated_properties.to_csv(aggregated_soil_properties_file, index=False, sep='\\t')\n","\n","# Save the full results (if GIPL supports multiple depth entries)\n","full_file_path =drive_path + '/per_time_instance_soil_proporties_input.txt'\n","thermal_properties.to_csv(full_file_path, index=False, sep='\\t')\n","\"\"\""],"metadata":{"id":"Mzx5UKUaD-R3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_path = drive_path + '/GIPLOriginalInputFiles'\n","\n","# Output files\n","updated_bound_file = input_path + \"/updated_bound.txt\"\n","updated_grid_file = input_path + \"/updated_grid.txt\"\n","updated_initial_file = input_path + \"/updated_initial.txt\"\n","updated_mineral_file = input_path + \"/updated_mineral.txt\"\n","updated_organic_file = input_path + \"/updated_organic.txt\"\n","updated_rsnow_file = input_path + \"/updated_rsnow.txt\"\n","updated_snow_file = input_path + \"/updated_snow.txt\"\n","updated_sites_file = input_path + \"/updated_sites.txt\""],"metadata":{"id":"1KFW2zwbfGWt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Bound.txt"],"metadata":{"id":"SymkKcqmgSod"}},{"cell_type":"code","source":["data = pd.read_csv(file_path)\n","\n","# Converting the Date column to datetime for proper resampling\n","data['Date'] = pd.to_datetime(data['Date'])\n","\n","# Resampling data to monthly intervals and averaging Air Temp\n","monthly_avg = data.resample('ME', on='Date')['Air Temp'].mean().reset_index()\n","monthly_avg['Air Temp'] = monthly_avg['Air Temp'] - 273.15\n","\n","# Creating the bound.txt structure\n","bound_txt = []\n","bound_txt.append(str(len(monthly_avg)))  # Number of observations (month intervals)\n","bound_txt += [f\"{i+1}\\t{temp:.3f}\" for i, temp in enumerate(monthly_avg['Air Temp'])]\n","\n","# Combine into a single string for the bound.txt content\n","bound_txt_content = \"\\n\".join(bound_txt)\n","\n","# Save to file for validation or further usage\n","output_path = updated_bound_file\n","with open(output_path, \"w\") as file:\n","    file.write(bound_txt_content)"],"metadata":{"id":"W4qYwVLdXCAR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Grid.txt"],"metadata":{"id":"nZH40y8qgXzo"}},{"cell_type":"code","source":["# Define above ground and below ground depths as per the new ranges\n","above_ground = np.arange(-1.5, 0, 0.05)  # Above ground depths in 0.05m steps\n","below_ground = np.concatenate([\n","    np.arange(0, 0.01, 0.005),  # Very fine intervals near surface\n","    np.arange(0.01, 0.1, 0.01),  # Slightly coarser intervals\n","    np.arange(0.1, 1, 0.02),  # Coarser intervals at intermediate depths\n","    np.arange(1, 20, 0.1)  # Coarser intervals for deeper layers\n","])\n","\n","# Combine above and below ground points\n","grid_points = np.concatenate((above_ground, below_ground))\n","grid_points = np.round(grid_points, 5)  # Match the precision in the example\n","\n","# Output points example from the reference file\n","output_points = [39, 48, 51, 55, 58, 62, 66, 70, 74, 81, 89, 97]\n","\n","# Prepare the grid.txt content\n","grid_txt = []\n","grid_txt.append(str(len(grid_points)))  # Total number of grid points\n","grid_txt += [f\"{point}\" for point in grid_points]  # Grid points\n","grid_txt.append(str(len(output_points)))  # Number of output points\n","grid_txt += [str(point) for point in output_points]  # Indices of output points\n","\n","# Combine into a single string for the grid.txt content\n","grid_txt_content = \"\\n\".join(grid_txt)\n","\n","# Save to file for validation or further usage\n","output_path = updated_grid_file\n","with open(output_path, \"w\") as file:\n","    file.write(grid_txt_content)"],"metadata":{"id":"6buutrvNgXRj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Initial.txt"],"metadata":{"id":"pxJWjUA_jgAT"}},{"cell_type":"code","source":["# Group by depth and month, selecting the first instance for each depth in a month\n","first_instance_data = data.sort_values(by=['Date']).groupby(['Depth', pd.Grouper(key='Date', freq='ME')]).first().reset_index()\n","\n","# For the initial condition, we take the first occurrence of each depth\n","initial_condition_data = first_instance_data.groupby('Depth').first().reset_index()\n","initial_condition_data['Avg Ground Temp'] = initial_condition_data['Avg Ground Temp'] - 273.15\n","\n","# Prepare initial.txt content\n","initial_txt = []\n","initial_txt.append(\"1   {}\".format(len(initial_condition_data)))  # First row with number of points\n","initial_txt.append(\" DEPTH  TEMP\")  # Header\n","initial_txt += [f\"{row.Depth:.3f}\\t{row['Avg Ground Temp']:.3f}\" for _, row in initial_condition_data.iterrows()]\n","\n","# Combine into a single string for the initial.txt content\n","initial_txt_content = \"\\n\".join(initial_txt)\n","\n","# Save to file for validation or further usage\n","output_path = updated_initial_file\n","with open(output_path, \"w\") as file:\n","    file.write(initial_txt_content)"],"metadata":{"id":"TLut9CnFjhHq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Mineral.txt"],"metadata":{"id":"f_5tnebhn5KR"}},{"cell_type":"code","source":["data = pd.read_csv(file_path)\n","\n","# Define depth ranges for each layer (in meters)\n","layer_ranges = [\n","    (0, 0.21),    # Layer 1\n","    (0.21, 0.36), # Layer 2\n","    (0.36, 0.96), # Layer 3\n","]\n","\n","# Initialize list to store layer properties\n","layers = []\n","\n","# Iterate over each depth range to calculate properties\n","for depth_range in layer_ranges:\n","    layer_data = data[(data['Depth'] >= depth_range[0]) & (data['Depth'] < depth_range[1])]\n","    if not layer_data.empty:\n","        # Calculate average Volumetric Water Content (WVC)\n","        wvc = layer_data['Soil Moisture'].mean()\n","        # Assign typical values for 'a' and 'b' coefficients\n","        a_coefficient = 0.05\n","        b_coefficient = -0.2\n","        # Assign standard volumetric heat capacities\n","        thawed_heat_capacity = 2.75e6  # J/(m³·K)\n","        frozen_heat_capacity = 1.75e6  # J/(m³·K)\n","        # Assign standard thermal conductivities\n","        thawed_conductivity = 1.5  # W/(m·K)\n","        frozen_conductivity = 2.0  # W/(m·K)\n","        # Calculate layer thickness\n","        thickness = depth_range[1] - depth_range[0]\n","        # Append the calculated properties to the layers list\n","        layers.append((\n","            wvc,\n","            a_coefficient,\n","            b_coefficient,\n","            thawed_heat_capacity,\n","            frozen_heat_capacity,\n","            thawed_conductivity,\n","            frozen_conductivity,\n","            thickness\n","        ))\n","\n","# Prepare the content for mineral.txt\n","mineral_txt = []\n","mineral_txt.append(\" 1\")  # First row, ignored by the model\n","mineral_txt.append(f\"1  {len(layers)}\")  # Second row: '1' ignored, second element is the number of layers\n","# Add each layer's properties\n","for layer in layers:\n","    line = \"\\t\".join(f\"{value:.5f}\" if isinstance(value, float) else str(value) for value in layer)\n","    mineral_txt.append(line)\n","\n","# Combine all lines into a single string\n","mineral_txt_content = \"\\n\".join(mineral_txt)\n","\n","# Save to mineral.txt file\n","output_path = updated_mineral_file\n","with open(output_path, \"w\") as file:\n","    file.write(mineral_txt_content)"],"metadata":{"id":"NtFdkIBAoCtM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Organic.txt (Identical as given)"],"metadata":{"id":"Xm5l2PSdpPAH"}},{"cell_type":"code","source":["organic_txt_content = \\\n","\"\"\" 1\n"," 1  0\n","\"\"\"\n","\n","# Save to file for validation or further usage\n","output_path = updated_organic_file\n","with open(output_path, \"w\") as file:\n","    file.write(organic_txt_content)"],"metadata":{"id":"FE__jc8cpiRh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Snow.txt (We do not have our own data, using the average value of their results)"],"metadata":{"id":"f-yUVE1Dpu-4"}},{"cell_type":"code","source":["data = pd.read_csv(file_path)\n","\n","# Ensure the 'Date' column is in datetime format\n","data['Date'] = pd.to_datetime(data['Date'], errors='coerce')\n","\n","# Extract unique months from the data\n","data['YearMonth'] = data['Date'].dt.to_period('M')  # Extract year and month as a period\n","unique_months = data['YearMonth'].unique()\n","unique_months_count = len(unique_months)  # Count unique months\n","\n","# Load the reference snow.txt to calculate the average snow depth\n","reference_snow_path = input_path + '/snow.txt'  # Replace with the reference snow.txt file path\n","with open(reference_snow_path, 'r') as file:\n","    reference_snow_data = file.readlines()\n","\n","# Extract snow depth values from the reference file (ignoring the first line)\n","reference_snow_depths = [\n","    float(line.split()[1]) for line in reference_snow_data[1:]\n","]\n","\n","# Calculate the average snow depth from the reference data\n","average_snow_depth = sum(reference_snow_depths) / len(reference_snow_depths)\n","\n","# Generate snow data with the correct number of observations and index numbers\n","snow_txt = [str(unique_months_count)]  # Number of observations\n","snow_txt += [f\"{i+1}\\t{average_snow_depth:.3f}\" for i in range(unique_months_count)]\n","\n","# Combine into a single string for snow.txt content\n","snow_txt_content = \"\\n\".join(snow_txt)\n","\n","# Save to file for validation or further usage\n","output_path = updated_snow_file  # Replace with your desired output path\n","with open(output_path, \"w\") as file:\n","    file.write(snow_txt_content)\n"],"metadata":{"id":"1GNQNSlSpnh-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##rsnow.txt (similar to above)"],"metadata":{"id":"321Makwjt4tL"}},{"cell_type":"code","source":["data = pd.read_csv(file_path)\n","\n","# Calculate the number of unique months in the CSV file\n","data['Date'] = pd.to_datetime(data['Date'], errors='coerce')  # Ensure 'Date' is datetime\n","data = data.dropna(subset=['Date'])  # Drop invalid dates\n","data['YearMonth'] = data['Date'].dt.to_period('M')  # Extract year and month\n","unique_months_count = data['YearMonth'].nunique()  # Count unique months\n","\n","# Set a constant thermal conductivity value from the reference rsnow.txt\n","constant_snow_conductivity = 0.3  # W/(mK), from reference, their example\n","\n","# Generate rsnow.txt content\n","rsnow_txt = [str(unique_months_count)]  # First line: number of observations\n","rsnow_txt += [f\"{i+1}\\t{constant_snow_conductivity:.3f}\" for i in range(unique_months_count)]\n","\n","# Combine into a single string for rsnow.txt content\n","rsnow_txt_content = \"\\n\".join(rsnow_txt)\n","\n","# Save to file for validation or further usage\n","output_path = updated_rsnow_file\n","with open(output_path, \"w\") as file:\n","    file.write(rsnow_txt_content)\n"],"metadata":{"id":"pXaxeWJUt6oo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##sites.txt (Nothing changed, just making it output identically just for reference)"],"metadata":{"id":"cy6b6LExuhnR"}},{"cell_type":"code","source":["# Generate sites.txt content using the example provided with temp_grd = 0.0\n","sites_txt_content = \"\"\"1\n","1   1   1   1   1   0.0\n","\"\"\"\n","\n","# Save to file for validation or further usage\n","output_path = updated_sites_file\n","with open(output_path, \"w\") as file:\n","    file.write(sites_txt_content)"],"metadata":{"id":"0unx3BzRunAF"},"execution_count":null,"outputs":[]}]}